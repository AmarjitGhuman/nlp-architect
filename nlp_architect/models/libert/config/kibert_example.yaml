max_epochs: 10
model_type: custom # bert or custom
base_init: [true, false] # true will randomly intitialize the embeddings which are injected (false is DPR/RAG embeddings)

data: lr # relative to nlp_architect/models/li_bert/data/preprocessed_conll
limit_data: 1.0
splits: 1
seeds: [7, 25, 26]
tag: # optional: name this experiment version
metric: asp_f1
baseline_str: custom_rnd_init
baseline_version: version_baseline

num_workers: 44
gpus: [0, 1]  # only run multi with overwrite_cache as false, otherwise multiple processes might try to write the cache file
parallel: true

# MODEL HPARAMS
custom_layers: [1, 2, 3]  # sets which layers to add the external embedding attention to (0 - 11)
add_norm: true  # keep this true, else training destabilizes
mlp: true  # whether or not to add 2-layer MLP between DPR/RAG embeddings and attention module

do_train: true
do_predict: true

# NEED TO OVERWRITE CACHE IF CHANGING RAG question/query, since RAG embeddings are stored in the cached feature file
# It's recommended to use the "create_kibert_features_file.py" script to create these feature files rather than the below option
overwrite_cache: false # if false, loads features from cached file (faster)
cache_dir: # default: nlp-architect/nlp_architect/cache
output_dir: # default: nlp-architect/nlp_architect/cache/li-bert-models
labels: labels.txt # relative to nlp_architect/models/li_bert/data

model_name_or_path: bert-base-uncased
train_batch_size: 8
eval_batch_size: 8
max_seq_length: 64
adam_epsilon: 1.0e-08
fast_dev_run: false
accumulate_grad_batches: 1
learning_rate: 5.0e-05
gradient_clip_val: 1.0
n_tpu_cores: 0
resume_from_checkpoint: null
tokenizer_name: null
val_check_interval: 1.0
warmup_steps: 0
weight_decay: 0.0
logger: true
